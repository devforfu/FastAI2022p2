# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_autograd.ipynb.

# %% auto 0
__all__ = ['MLP', 'softmax', 'ce', 'log_softmax', 'Dataset', 'Sampler', 'SequentialSampler', 'RandomSampler', 'DataLoader', 'acc',
           'Optimizer', 'accuracy', 'fit', 'get_dls']

# %% ../nbs/03_autograd.ipynb 3
from collections import defaultdict
from dataclasses import dataclass, field
from functools import reduce
from typing import Callable, Protocol

# %% ../nbs/03_autograd.ipynb 4
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F

# %% ../nbs/03_autograd.ipynb 5
from tensorviewer import tv
from utils import get_mnist

# %% ../nbs/03_autograd.ipynb 12
class MLP(nn.Module):
    def __init__(self, n_i, n_h, n_o):
        super().__init__()
        self.layers = nn.ModuleList([nn.Linear(n_i, n_h), nn.ReLU(), nn.Linear(n_h, n_o)])
    def forward(self, x):
        return reduce(lambda x, l: l(x), self.layers, x)

# %% ../nbs/03_autograd.ipynb 16
def softmax(t):
    p = torch.exp(t)
    return p / p.sum(1, keepdim=True)

# %% ../nbs/03_autograd.ipynb 19
def ce(logits, targets):
    probs = softmax(logits)
    probs = probs.gather(dim=1, index=targets.view(-1, 1)).squeeze(1)
    return -probs.log().mean()

# %% ../nbs/03_autograd.ipynb 22
def log_softmax(t):
    t.sub_(t.max(dim=1, keepdim=True)[0])
    t.sub_(t.exp().sum(dim=1, keepdim=True).log())
    return t

# %% ../nbs/03_autograd.ipynb 23
def ce(logits, targets):
    return -log_softmax(logits).gather(dim=1, index=targets.view(-1, 1)).squeeze(1).mean()

# %% ../nbs/03_autograd.ipynb 26
class Dataset:
    def __init__(self, x, y):
        assert len(x) == len(y)
        self.x, self.y = x, y
    def __len__(self): return len(self.x)
    def __getitem__(self, index): return self.x[index], self.y[index]

# %% ../nbs/03_autograd.ipynb 30
class Sampler(Protocol):
    def get_idx(self, dataset: Dataset): pass

@dataclass
class SequentialSampler(Sampler):
    def get_idx(self, dataset: Dataset): return np.arange(len(dataset))

@dataclass
class RandomSampler(Sampler):
    def get_idx(self, dataset: Dataset): 
        idx = SequentialSampler().get_idx(dataset)
        np.random.shuffle(idx)
        return idx
    
@dataclass
class DataLoader:
    dataset: Dataset
    bs: int = 1
    shuffle: bool = False
    sampler: Sampler = None
    
    def __post_init__(self):
        if self.sampler is None:
            self.sampler = (RandomSampler if self.shuffle else SequentialSampler)()
    
    def __iter__(self):
        idx = self.sampler.get_idx(self.dataset)
        for i in range(0, len(idx), self.bs):
            yield self.dataset[idx[i:i+self.bs]]
            
    def __len__(self): return int(np.ceil(len(self.dataset) // self.bs))

# %% ../nbs/03_autograd.ipynb 34
def acc(x, y): return (x == y).float().mean()

# %% ../nbs/03_autograd.ipynb 35
class Optimizer:
    def __init__(self, params, lr): 
        self.params, self.lr = list(params), lr
    def step(self): 
        for param in self.params:
            param.data -= self.lr * param.grad
    def zero_grad(self):
        for param in self.params:
            param.grad.zero_()

# %% ../nbs/03_autograd.ipynb 39
def _fit(
    epochs: int, 
    model: nn.Module, 
    loss_fn: Callable,
    optimizer: optim.Optimizer | Optimizer,
    train_dl: DataLoader,
    valid_dl: DataLoader,
):
    for epoch in range(epochs):
        cnts = defaultdict(int)
        for (xb, yb) in train_dl:
            loss = loss_fn(model(xb), yb)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            cnts["train_loss"] += loss.item()
        cnts["train_loss"] /= len(train_dl)
        with torch.no_grad():
            for (xb, yb) in valid_dl:
                logits = model(xb)
                pred = logits.softmax(dim=-1).argmax(dim=-1)
                cnts["valid_loss"] += loss_fn(logits, yb)
                cnts["valid_acc"] += acc(pred, yb)
        cnts["valid_loss"] /= len(valid_dl)
        cnts["valid_acc"] /= len(valid_dl)
        cnts["epoch"] = epoch
        print("Epoch {epoch:03d} | "
              "loss(trn) = {train_loss:.4f} | "
              "loss(val) = {valid_loss:.4f} | "
              "acc = {valid_acc:.4f}".format(**cnts))

# %% ../nbs/03_autograd.ipynb 40
def accuracy(out, yb): return (out.argmax(dim=1)==yb).float().mean()

# %% ../nbs/03_autograd.ipynb 41
def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
    for epoch in range(epochs):
        model.train()
        for xb,yb in train_dl:
            loss = loss_func(model(xb), yb)
            loss.backward()
            opt.step()
            opt.zero_grad()

        model.eval()
        with torch.no_grad():
            tot_loss,tot_acc,count = 0.,0.,0
            for xb,yb in valid_dl:
                pred = model(xb)
                n = len(xb)
                count += n
                tot_loss += loss_func(pred,yb).item()*n
                tot_acc  += accuracy (pred,yb).item()*n
        print(epoch, tot_loss/count, tot_acc/count)
    return tot_loss/count, tot_acc/count

# %% ../nbs/03_autograd.ipynb 42
def get_dls(train_ds, valid_ds, bs, **kwargs):
    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),
            DataLoader(valid_ds, batch_size=bs*2, **kwargs))
